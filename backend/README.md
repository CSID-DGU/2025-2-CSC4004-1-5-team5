# 🎧 Soribom Backend — Django REST API

AI 기반 지하철 안내방송 인식 및 키워드 감지 서비스를 담당하는 백엔드입니다.  
본 서버는 **Django REST Framework + Celery + Redis** 기반으로 동작하며,  
AI 서버(Flask, PyTorch)와 연동되어 녹음된 음성의 분석·요약을 처리합니다.

---

## 🧩 1. Entity 설계

![제목 없음 (2)](https://github.com/user-attachments/assets/6bb71b61-25b4-4a45-a7b7-58918b197df1)
> 스키마 설계 개요 — Session을 중심으로 Keyword, AudioChunk, Broadcast, Transcript, Alert 관계 관리

<img width="990" height="643" alt="image" src="https://github.com/user-attachments/assets/a3b5f652-e31b-40d8-8096-ec0130139c16" />
> ERD — 세션 단위 데이터 흐름 (SQLite 기반)

본 서비스의 데이터베이스는 **SQLite**를 사용하며,  
모델은 `Session`을 중심으로 **녹음 → 분석 → 감지 → 요약** 과정을 유기적으로 관리하도록 설계되었습니다.

- **Session**: 사용자가 녹음을 시작할 때 생성되는 단위  
- **AudioChunk**: 10초 단위 녹음 파일  
- **Broadcast**: Whisper 모델이 청크를 분석해 통합한 안내방송 결과  
- **Keyword**: 사용자가 등록한 감지 단어  
- **Alert**: 방송 내에서 키워드 감지 시 생성되는 로그  
- **Transcript**: 세션 단위의 전체 요약 결과

> 비로그인 환경에서도 세션 단위로 전체 음성 인식 및 감지 흐름을 효율적으로 관리할 수 있도록 구성됨

---

## 🎙️ 2. 오디오 업로드 처리 흐름

오디오 파일은 `audio/` API를 통해 업로드되며,  
업로드 직후 비동기 **Celery Task**로 AI 서버(Flask, PyTorch)에 전송되어 처리됩니다.

```plaintext
AudioChunk 저장
↓
Celery Task → AI 서버 전송
↓
소음 분리 (DCCRNet)
↓
Whisper STT → Broadcast 생성
↓
키워드 감지 → SSE 알림
↓
진행률 갱신
```

## 🧠 2.3 안내방송 복원 및 방송 그룹핑

안내방송 복원은 **실시간이 아닌 결과 조회 시점(`session/{id}/results/`)** 에 수행됩니다.  
실시간 중에는 빠른 키워드 감지를 위해 Whisper STT만 수행하고,  
결과 조회 시 GPT-4o-mini를 이용해 **문장 복원, 그룹핑, 요약**을 수행합니다.

| 시점 | 수행 로직 | 목적 |
|------|-------------|------|
| **실시간 중** | Whisper STT 변환만 수행 | 속도 우선 / 빠른 키워드 감지 |
| **결과 조회 시** | GPT-4o-mini 기반 복원·그룹핑·요약 | 품질 우선 / 자연스러운 문장 복원 |

---

### ① 안내방송 복원 (LLM 교정)

STT 결과는 10초 단위로 잘려 들어오기 때문에  
문장이 중간에서 끊기거나 단어가 왜곡되는 경우가 많습니다.  
LLM은 이를 보정하여 **자연스러운 지하철 안내방송체 문장**으로 복원합니다.

> 예시  
> “이번 역은 구” → “이번 역은 구로역입니다.”  

**복원 로직 주요 단계**
- 끊긴 문장 연결 및 중복 문장 제거  
- 문장 순서 재배열  
- 실제 안내방송 톤으로 교정 (“이번 역은”, “내리실 문은 오른쪽입니다.” 등)

---

### ② 방송 요약 및 구조화

복원된 안내방송 문장은 LLM(GPT-4o-mini)을 통해  
**정형화된 JSON 형태로 요약**됩니다.  
이는 방송 내용을 네 가지 핵심 항목으로 분리하여 저장합니다.

| 항목 | 예시 | 설명 |
|------|------|------|
| `station` | `"강남역"` | 방송 대상 역 이름 |
| `door_direction` | `"오른쪽"` | 문 열림 방향 |
| `transfer` | `"2호선 환승 가능"` | 환승 정보 |
| `notice` | `"안전 문구"` | 안전·주의 방송 내용 |

---

### ③ 방송 연속성 판단

하나의 안내방송은 여러 개의 오디오 청크로 구성될 수 있습니다.  
따라서 각 청크가 같은 방송인지, 혹은 다른 방송인지 구분해야 합니다.  

이를 위해 다음 **6단계 규칙 기반 로직**을 적용합니다.

| 우선순위 | 규칙 | 설명 |
|-----------|------|------|
| ① | **역명 기반 비교** | 추출된 역명이 동일하면 같은 방송으로 판단 |
| ② | **‘이번 역은’ 탐지** | 새 역 안내 시작 시 새로운 방송 처리 |
| ③ | **시간 간격(≤11초)** | 짧은 간격일 경우 동일 방송으로 간주 |
| ④ | **문장 완성도 판단** | 미완성 문장은 다음 청크와 연결 |
| ⑤ | **안내방송 키워드 기반** | `이번 역`, `열차`, `환승`, `내리실 문` 등이 연속 등장 시 연결 |
| ⑥ | **LLM 문맥 판단** | 위 규칙으로 불명확할 경우 GPT가 직접 연속 여부 판별 |

---

### ④ 역명 및 문맥 추론

보조적으로, STT 텍스트 내에서 **역명 후보를 자동 탐지**합니다.  
이를 통해 방송 간의 일관성을 유지하고, LLM이 추측으로 역명을 생성하는 것을 방지합니다.

- “이번 역은 …역입니다.” 패턴 인식  
- 잘못 인식된 단어 교정 (예: “입은” → “이번”, “오른죽” → “오른쪽”)  
- 문맥 내에서 의미 유추 및 보정 수행

---

### ⑤ 그룹핑 및 요약 저장

그룹핑된 방송들은 **타임라인(timeline)** 형태로 구성되어  
세션 내 안내방송 단위별로 정리됩니다.

| 항목 | 설명 |
|------|------|
| `announcement_id` | 방송 그룹 고유 ID |
| `broadcast_ids` | 해당 그룹에 포함된 Broadcast 목록 |
| `audio_chunks` | 해당 방송이 사용한 AudioChunk 목록 |
| `full_text` | LLM 교정 후 완성된 문장 |
| `summary` | 방송 단위 요약 결과 |
| `info` | 구조화된 정보(JSON) |
| `keywords_detected` | 감지된 키워드 목록 |
| `confidence_avg` | 방송별 인식 신뢰도 평균 |

이 모든 결과는 **Transcript 모델 엔티티**에 저장되며,  
사용자는 세션 종료 후 결과 화면에서  
**“요약 타임라인 + 감지 키워드”** 형태로 확인할 수 있습니다.


---
## 📡 3. SSE 실시간 이벤트 구조

서버는 세션 진행 중 발생하는 주요 이벤트를  
**SSE(Server-Sent Events)** 형태로 프론트엔드에 **실시간 전송**합니다.  
이를 통해 사용자에게 즉각적인 상태 변화와 감지 알림을 제공합니다.

| 이벤트 타입 | 설명 |
|--------------|------|
| **`status`** | 세션 상태 업데이트 (`RECORDING`, `COMPLETE`) |
| **`chunk_received`** | 새로운 오디오 청크 업로드 시 전송 |
| **`chunk_count`** | 현재까지 처리 완료된 청크 개수 전송 |
| **`keyword_alert`** | 등록된 키워드가 감지되었을 때 알림 발생 |

프론트엔드는 `EventSource`를 통해 해당 이벤트를 수신하며,  
`keyword_alert` 발생 시 실시간으로 사용자 화면에 알림을 표시합니다.

> 📡 SSE는 WebSocket 대비 단방향이지만, 서버에서 다수의 클라이언트로 이벤트를  
> 안정적으로 전송하기에 적합하며, 본 프로젝트에서는 녹음 세션 진행 상황을  
> 실시간으로 브로드캐스트하는 데 활용되었습니다.

---

## 🧩 4. API 명세 요약

소리봄 백엔드는 RESTful 구조를 기반으로 하며,  
세션 생성부터 결과 조회까지의 모든 기능을 API 형태로 제공합니다.  

> **주요 도메인:** `Session`, `Keyword`, `Audio`, `Results`

| 분류 | 메서드 | 엔드포인트 | 설명 |
|------|---------|-------------|------|
| **Session** | `POST` | `/api/session/` | 비로그인 세션 생성 |
|  | `DELETE` | `/api/session/{id}/` | 세션 종료 및 삭제 |
|  | `GET` | `/api/session/{id}/status/` | 세션 상태 및 진행률 조회 |
|  | `GET` | `/api/session/{id}/results/` | 세션별 전체 요약 결과 조회 |
| **Keyword** | `GET` | `/api/keywords?session_id={id}` | 세션별 키워드 목록 조회 |
|  | `POST` | `/api/keywords/` | 감지 키워드 등록 |
|  | `DELETE` | `/api/keywords/{id}/` | 키워드 삭제 |
| **Audio** | `POST` | `/api/audio/` | 10초 단위 오디오 청크 업로드 |
| **Stream** | `GET` | `/api/session/{id}/stream/` | SSE 실시간 이벤트 스트림 연결 |

> 🧠 AI 서버(Flask, PyTorch)는 `/enhance_stt` 엔드포인트로 연결되어 있으며,  
> Django Celery Worker를 통해 비동기적으로 호출됩니다.

---

## 🔄 5. 데이터 처리 흐름 요약

소리봄의 전체 파이프라인은 다음과 같이 구성됩니다.  
`녹음 → 분석 → 감지 → 요약`의 흐름이 **Session 단위**로 자동 관리됩니다.

```plaintext
[Session 생성]
   ↓
[Keyword 등록]
   ↓
[AudioChunk 업로드 → SSE 전송]
   ↓
[Celery 비동기 STT 처리 → Broadcast 생성]
   ↓
[Keyword 감지(Alert) → SSE 알림]
   ↓
[안내방송 복원 + 그룹핑 → Transcript 저장]
   ↓
[결과 조회 및 요약 출력]
````

> 모든 처리는 비동기로 수행되어 실시간 알림과
> 고품질 안내방송 복원을 동시에 지원합니다.

---

📘 **작성자**
**하연희** — Server, Leader



