{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t_Y_m5A2cdoE"
      },
      "outputs": [],
      "source": [
        "# 1. 라이브러리 설치\n",
        "!pip install -q transformers datasets peft bitsandbytes accelerate jiwer librosa soundfile\n",
        "\n",
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import gc\n",
        "import torch\n",
        "import librosa\n",
        "import numpy as np\n",
        "from dataclasses import dataclass\n",
        "from typing import Any\n",
        "from datasets import Dataset\n",
        "from google.colab import drive\n",
        "\n",
        "from transformers import (\n",
        "    WhisperFeatureExtractor,\n",
        "    WhisperTokenizer,\n",
        "    WhisperProcessor,\n",
        "    WhisperForConditionalGeneration,\n",
        "    Seq2SeqTrainingArguments,\n",
        "    Seq2SeqTrainer,\n",
        "    BitsAndBytesConfig,\n",
        "    EarlyStoppingCallback\n",
        ")\n",
        "from peft import (\n",
        "    prepare_model_for_kbit_training,\n",
        "    LoraConfig,\n",
        "    get_peft_model,\n",
        "    PeftModel\n",
        ")\n",
        "\n",
        "# =========================================================\n",
        "# Config 설정\n",
        "# =========================================================\n",
        "class Config:\n",
        "    # 데이터셋 경로\n",
        "    ZIP_PATH = \"\" #경로 지정 필요\n",
        "    DATA_ROOT = \"\" #경로 지정 필요\n",
        "\n",
        "    # 학습된 LoRA 어댑터 저장 경로\n",
        "    LORA_OUTPUT_DIR = \"\" #경로 지정 필요\n",
        "\n",
        "    # 최종 병합된 모델 저장 경로\n",
        "    MERGED_OUTPUT_DIR = \"\" #경로 지정 필요\n",
        "\n",
        "    # 모델 ID\n",
        "    MODEL_ID = \"openai/whisper-large-v3\"\n",
        "    LANGUAGE = \"ko\"\n",
        "    TASK = \"transcribe\"\n",
        "\n",
        "# =========================================================\n",
        "# GPU Check\n",
        "# =========================================================\n",
        "print(\"GPU 정보:\", torch.cuda.get_device_name(0))\n",
        "total_vram = torch.cuda.get_device_properties(0).total_memory / (1024**3)\n",
        "print(f\"총 VRAM: {total_vram:.2f} GB\")\n",
        "\n",
        "if total_vram <= 16:\n",
        "    print(\"T4 (16GB) 환경 감지 -> 4bit QLoRA 설정 적용\")\n",
        "\n",
        "# =========================================================\n",
        "# Step 1: 데이터셋 준비\n",
        "# =========================================================\n",
        "print(\"Step 1: 데이터셋 준비 중...\")\n",
        "if not os.path.exists('/content/drive'): drive.mount('/content/drive')\n",
        "\n",
        "if os.path.exists(Config.DATA_ROOT): shutil.rmtree(Config.DATA_ROOT)\n",
        "shutil.unpack_archive(Config.ZIP_PATH, Config.DATA_ROOT)\n",
        "\n",
        "with open(f\"{Config.DATA_ROOT}/metadata.json\", 'r', encoding='utf-8') as f:\n",
        "    metadata = json.load(f)\n",
        "\n",
        "dataset = Dataset.from_dict({\n",
        "    \"audio\": [os.path.join(Config.DATA_ROOT, \"audio\", i['file_name']) for i in metadata],\n",
        "    \"sentence\": [i['text'] for i in metadata]\n",
        "})\n",
        "\n",
        "processor = WhisperProcessor.from_pretrained(Config.MODEL_ID, language=Config.LANGUAGE, task=Config.TASK)\n",
        "feature_extractor = processor.feature_extractor\n",
        "tokenizer = processor.tokenizer\n",
        "\n",
        "def prepare_dataset(batch):\n",
        "    try:\n",
        "        audio, _ = librosa.load(batch[\"audio\"], sr=16000)\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {batch['audio']}: {e}\")\n",
        "        audio = np.zeros(16000)\n",
        "\n",
        "    batch[\"input_features\"] = feature_extractor(audio, sampling_rate=16000).input_features[0]\n",
        "    batch[\"labels\"] = tokenizer(batch[\"sentence\"]).input_ids\n",
        "    return batch\n",
        "\n",
        "dataset = dataset.map(prepare_dataset, num_proc=1)\n",
        "dataset = dataset.train_test_split(test_size=0.1)\n",
        "\n",
        "print(f\"데이터 준비 완료: Train {len(dataset['train'])}개 / Test {len(dataset['test'])}개\")\n",
        "\n",
        "@dataclass\n",
        "class DataCollator:\n",
        "    processor: Any\n",
        "    def __call__(self, features):\n",
        "        input_features = [{\"input_features\": f[\"input_features\"]} for f in features]\n",
        "        batch = self.processor.feature_extractor.pad(input_features, return_tensors=\"pt\")\n",
        "\n",
        "        label_features = [{\"input_ids\": f[\"labels\"]} for f in features]\n",
        "        labels_batch = self.processor.tokenizer.pad(label_features, return_tensors=\"pt\")\n",
        "\n",
        "        labels = labels_batch[\"input_ids\"].masked_fill(labels_batch.attention_mask.ne(1), -100)\n",
        "\n",
        "        if (labels[:, 0] == self.processor.tokenizer.bos_token_id).all().cpu().item():\n",
        "            labels = labels[:, 1:]\n",
        "\n",
        "        batch[\"labels\"] = labels\n",
        "        return batch\n",
        "\n",
        "# =========================================================\n",
        "# Step 2: 학습 (Training)\n",
        "# =========================================================\n",
        "print(\"Step 2: 모델 로딩 및 학습 시작 (4bit QLoRA)\")\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_use_double_quant=True\n",
        ")\n",
        "\n",
        "model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    Config.MODEL_ID,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "lora_config = LoraConfig(\n",
        "    r=32,\n",
        "    lora_alpha=64,\n",
        "    target_modules=[\"q_proj\", \"v_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n",
        "\n",
        "os.environ[\"WANDB_DISABLED\"] = \"true\"\n",
        "\n",
        "training_args = Seq2SeqTrainingArguments(\n",
        "    output_dir=Config.LORA_OUTPUT_DIR,\n",
        "    per_device_train_batch_size=2,\n",
        "    gradient_accumulation_steps=8,\n",
        "    learning_rate=1e-4,\n",
        "    warmup_steps=50,\n",
        "    max_steps=500,  # 필요에 따라 조절\n",
        "    fp16=True,\n",
        "    eval_strategy=\"steps\",\n",
        "    per_device_eval_batch_size=2,\n",
        "    predict_with_generate=True,\n",
        "    generation_max_length=225,\n",
        "    save_steps=50,\n",
        "    eval_steps=50,\n",
        "    logging_steps=25,\n",
        "    load_best_model_at_end=True,\n",
        "    metric_for_best_model=\"loss\",\n",
        "    greater_is_better=False,\n",
        "    remove_unused_columns=False\n",
        ")\n",
        "\n",
        "trainer = Seq2SeqTrainer(\n",
        "    args=training_args,\n",
        "    model=model,\n",
        "    train_dataset=dataset[\"train\"],\n",
        "    eval_dataset=dataset[\"test\"],\n",
        "    data_collator=DataCollator(processor=processor),\n",
        "    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "\n",
        "# 학습 결과(LoRA 어댑터) 저장\n",
        "print(f\"LoRA 어댑터 저장 중: {Config.LORA_OUTPUT_DIR}\")\n",
        "model.save_pretrained(Config.LORA_OUTPUT_DIR)\n",
        "processor.save_pretrained(Config.LORA_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(Config.LORA_OUTPUT_DIR)\n",
        "\n",
        "print(\"학습 완료.\")\n",
        "\n",
        "# =========================================================\n",
        "# Step 3: 메모리 정리 (Merge 전 필수)\n",
        "# =========================================================\n",
        "print(\"Step 3: Merge를 위해 메모리 정리 중...\")\n",
        "\n",
        "# 학습에 사용된 모델 객체 삭제\n",
        "del model\n",
        "del trainer\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "\n",
        "print(f\"VRAM 확보 완료. 현재 메모리: {torch.cuda.memory_allocated() / 1024**3:.2f} GB\")\n",
        "\n",
        "# =========================================================\n",
        "# Step 4: 모델 병합 (Merge LoRA + Base)\n",
        "# =========================================================\n",
        "print(\"Step 4: 모델 병합 (Base Model + LoRA Adapter)\")\n",
        "\n",
        "# 4-1. Base Model 로딩 (FP16)\n",
        "print(\"Base Whisper 모델 로딩 중 (FP16)...\")\n",
        "base_model = WhisperForConditionalGeneration.from_pretrained(\n",
        "    Config.MODEL_ID,\n",
        "    torch_dtype=torch.float16,\n",
        "    device_map=\"auto\"\n",
        ")\n",
        "\n",
        "# 4-2. LoRA 어댑터 로딩\n",
        "print(f\"LoRA 어댑터 로딩 중: {Config.LORA_OUTPUT_DIR}\")\n",
        "lora_model = PeftModel.from_pretrained(\n",
        "    base_model,\n",
        "    Config.LORA_OUTPUT_DIR\n",
        ")\n",
        "\n",
        "# 4-3. Merge 진행\n",
        "print(\"Merge 진행 중...\")\n",
        "merged_model = lora_model.merge_and_unload()\n",
        "\n",
        "# 4-4. 최종 모델 저장\n",
        "print(f\"Merge된 모델 저장 중: {Config.MERGED_OUTPUT_DIR}\")\n",
        "merged_model.save_pretrained(\n",
        "    Config.MERGED_OUTPUT_DIR,\n",
        "    safe_serialization=True\n",
        ")\n",
        "\n",
        "# 토크나이저와 설정 파일도 함께 저장 (독립 사용을 위해 필수)\n",
        "processor.save_pretrained(Config.MERGED_OUTPUT_DIR)\n",
        "tokenizer.save_pretrained(Config.MERGED_OUTPUT_DIR)\n",
        "\n",
        "print(f\"모든 작업 완료, 병합된 모델 경로: {Config.MERGED_OUTPUT_DIR}\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
