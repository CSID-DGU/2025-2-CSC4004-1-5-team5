{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IVl7Kp4w_Kn"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------------------\n",
        "# 1. 라이브러리 설치 및 임포트\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"--- 1. 라이브러리 설치 및 임포트 중... ---\")\n",
        "!pip install -q asteroid pyunpack patool torch-stoi\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.functional as F\n",
        "import torchaudio.transforms as T\n",
        "from torch.utils.data import Dataset, DataLoader, Subset\n",
        "from pathlib import Path\n",
        "from tqdm import tqdm\n",
        "from pyunpack import Archive\n",
        "\n",
        "# Google Drive\n",
        "from google.colab import drive\n",
        "\n",
        "# Asteroid & Custom Loss\n",
        "from asteroid.models import ConvTasNet\n",
        "from asteroid.losses import PITLossWrapper\n",
        "from torch_stoi import NegSTOILoss\n",
        "\n",
        "print(\"라이브러리 준비 완료\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 2. 경로 및 하이퍼파라미터 설정 (전역 설정)\n",
        "# -----------------------------------------------------------------------------\n",
        "# Drive 마운트\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# --- 경로 설정 ---\n",
        "# 본인 환경에 맞게 수정이 필요\n",
        "DRIVE_PATH = \"\" #경로 지정 필요\n",
        "MODEL_PATH = \"\" #경로 지정 필요\n",
        "ZIP_PATH = os.path.join(MODEL_PATH, \"zips\")\n",
        "LOCAL_DATA_PATH = \"/content/\"\n",
        "TRAIN_DATA_PATH = os.path.join(LOCAL_DATA_PATH, \"train/\")\n",
        "VAL_DATA_PATH = os.path.join(LOCAL_DATA_PATH, \"val/\")\n",
        "\n",
        "# 모델 저장 파일명\n",
        "CHECKPOINT_PATH = os.path.join(MODEL_PATH, \"convtasnet_latest_checkpoint.pth\")\n",
        "BEST_MODEL_PATH = os.path.join(MODEL_PATH, \"convtasnet_best_model.pth\")\n",
        "\n",
        "# --- 하이퍼파라미터 ---\n",
        "NUM_EPOCHS = 50\n",
        "BATCH_SIZE = 2\n",
        "SAMPLE_RATE = 16000\n",
        "SEGMENT_DURATION_SEC = 3\n",
        "TRAIN_STEPS_PER_EPOCH = 1000   # 훈련 시 1 Epoch당 생성할 배치 수\n",
        "MAX_VAL_SAMPLES = 200          # 검증 시 사용할 고정 샘플 수\n",
        "EPSILON = 1e-8\n",
        "LOSS_LAMBDA_V3 = 0.8           # SI-SNR 가중치\n",
        "\n",
        "# GPU 설정\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"사용할 장치: {device}\")\n",
        "\n",
        "# 로컬 폴더 생성\n",
        "os.makedirs(TRAIN_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(VAL_DATA_PATH, exist_ok=True)\n",
        "os.makedirs(MODEL_PATH, exist_ok=True)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 3. 데이터 압축 해제 (ZIP 처리)\n",
        "# -----------------------------------------------------------------------------\n",
        "def extract_zip_safe_py(zip_file):\n",
        "    \"\"\"한글/긴 파일명 안전 압축 해제 및 자동 분류\"\"\"\n",
        "    zip_name = os.path.basename(zip_file)\n",
        "\n",
        "    if zip_name == \"train_안내방송.zip\":\n",
        "        dst_dir = os.path.join(TRAIN_DATA_PATH, \"안내방송\")\n",
        "    else:\n",
        "        zip_name_lower = zip_name.lower()\n",
        "        if \"train\" in zip_name_lower:\n",
        "            dst_dir = TRAIN_DATA_PATH\n",
        "        elif \"val\" in zip_name_lower:\n",
        "            dst_dir = VAL_DATA_PATH\n",
        "        else:\n",
        "            dst_dir = LOCAL_DATA_PATH\n",
        "\n",
        "    os.makedirs(dst_dir, exist_ok=True)\n",
        "    try:\n",
        "        Archive(zip_file).extractall(dst_dir)\n",
        "    except Exception as e:\n",
        "        print(f\"[Warning] 압축 해제 실패: {zip_name} / 에러: {e}\")\n",
        "\n",
        "if os.path.exists(ZIP_PATH):\n",
        "    zip_files = [f for f in os.listdir(ZIP_PATH) if f.endswith(\".zip\")]\n",
        "    if not zip_files:\n",
        "        print(\"[Warning] ZIP 파일을 찾을 수 없습니다.\")\n",
        "    else:\n",
        "        print(f\"총 {len(zip_files)}개의 ZIP 파일 압축 해제 시작...\")\n",
        "        for zip_name in tqdm(zip_files, desc=\"압축 해제 중\"):\n",
        "            src_zip = os.path.join(ZIP_PATH, zip_name)\n",
        "            extract_zip_safe_py(src_zip)\n",
        "        print(\"모든 데이터 압축 해제 완료\")\n",
        "else:\n",
        "    print(f\"[Warning] ZIP 경로 없음: {ZIP_PATH} (데이터가 이미 풀려있다면 무시하세요)\")\n",
        "\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 4. 오디오 처리 헬퍼 함수 및 Dataset 정의\n",
        "# -----------------------------------------------------------------------------\n",
        "def load_audio(path, sample_rate=16000):\n",
        "    try:\n",
        "        waveform, sr = torchaudio.load(path)\n",
        "    except Exception as e:\n",
        "        return torch.zeros((1, sample_rate))\n",
        "\n",
        "    if sr != sample_rate:\n",
        "        resampler = T.Resample(orig_freq=sr, new_freq=sample_rate)\n",
        "        waveform = resampler(waveform)\n",
        "\n",
        "    if waveform.shape[0] > 1:\n",
        "        waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "    if waveform.shape[1] == 0:\n",
        "        return torch.zeros((1, sample_rate))\n",
        "    return waveform\n",
        "\n",
        "def pad_or_trim(waveform, target_len):\n",
        "    current_len = waveform.shape[1]\n",
        "    if current_len > target_len:\n",
        "        start = random.randint(0, current_len - target_len)\n",
        "        return waveform[:, start : start + target_len]\n",
        "    elif current_len < target_len:\n",
        "        return torch.nn.functional.pad(waveform, (0, target_len - current_len))\n",
        "    else:\n",
        "        return waveform\n",
        "\n",
        "def mix_audio(signal, noise, snr_db):\n",
        "    signal_power = signal.norm(p=2)\n",
        "    noise_power = noise.norm(p=2)\n",
        "    if signal_power == 0 or noise_power == 0:\n",
        "        return signal + noise\n",
        "    snr = 10 ** (snr_db / 10)\n",
        "    scale = (signal_power / (noise_power * snr)) ** 0.5\n",
        "    return signal + (noise * scale)\n",
        "\n",
        "class SoribomDataset(Dataset):\n",
        "    def __init__(self, data_root_path, sample_rate=16000, segment_duration_sec=10,\n",
        "                 mode='train', steps_per_epoch=None, batch_size=1):\n",
        "        self.data_root = Path(data_root_path)\n",
        "        self.sample_rate = sample_rate\n",
        "        self.segment_len = sample_rate * segment_duration_sec\n",
        "        self.mode = mode\n",
        "        self.steps_per_epoch = steps_per_epoch\n",
        "        self.batch_size = batch_size\n",
        "\n",
        "        print(f\"[{mode.upper()}] 데이터셋 스캔 중: {self.data_root}\")\n",
        "        self.announcements = self._find_files(self.data_root / '안내방송')\n",
        "        self.dialogues = self._find_files(self.data_root / '대화소음')\n",
        "        self.environments = self._find_files(self.data_root / '환경소음')\n",
        "        self.rirs = self._find_files(self.data_root / '공간음향')\n",
        "\n",
        "        print(f\"  - 안내방송: {len(self.announcements)}개\")\n",
        "        if not self.announcements: print(\"[Warning] 안내방송 파일이 없습니다.\")\n",
        "\n",
        "    def _find_files(self, path):\n",
        "        return list(path.rglob('*.wav')) + list(path.rglob('*.mp3')) + list(path.rglob('*.flac'))\n",
        "\n",
        "    def __len__(self):\n",
        "        if self.mode == 'train' and self.steps_per_epoch:\n",
        "            return self.steps_per_epoch * self.batch_size\n",
        "        else:\n",
        "            return len(self.announcements)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # 1. 소스 오디오 로드\n",
        "        if self.mode == 'train':\n",
        "            target_speech = load_audio(random.choice(self.announcements), self.sample_rate)\n",
        "            try: dialogue_noise = load_audio(random.choice(self.dialogues), self.sample_rate)\n",
        "            except: dialogue_noise = torch.zeros((1, self.segment_len))\n",
        "            try: env_noise = load_audio(random.choice(self.environments), self.sample_rate)\n",
        "            except: env_noise = torch.zeros((1, self.segment_len))\n",
        "            try: rir = load_audio(random.choice(self.rirs), self.sample_rate)\n",
        "            except: rir = torch.tensor([[1.0]])\n",
        "        else: # Validation/Eval\n",
        "            target_speech = load_audio(self.announcements[idx], self.sample_rate)\n",
        "            try: dialogue_noise = load_audio(self.dialogues[idx % len(self.dialogues)], self.sample_rate)\n",
        "            except: dialogue_noise = torch.zeros((1, self.segment_len))\n",
        "            try: env_noise = load_audio(self.environments[idx % len(self.environments)], self.sample_rate)\n",
        "            except: env_noise = torch.zeros((1, self.segment_len))\n",
        "            try: rir = load_audio(self.rirs[idx % len(self.rirs)], self.sample_rate)\n",
        "            except: rir = torch.tensor([[1.0]])\n",
        "\n",
        "        # 2. 전처리 (길이 맞추기, Reverb)\n",
        "        target_speech = pad_or_trim(target_speech, self.segment_len)\n",
        "        dialogue_noise = pad_or_trim(dialogue_noise, self.segment_len)\n",
        "        env_noise = pad_or_trim(env_noise, self.segment_len)\n",
        "\n",
        "        rir = rir / torch.norm(rir, p=2)\n",
        "        if rir.shape[1] > self.sample_rate: rir = rir[:, :self.sample_rate]\n",
        "\n",
        "        target_reverb = torchaudio.functional.fftconvolve(target_speech, rir, mode='same')\n",
        "        dialogue_reverb = torchaudio.functional.fftconvolve(dialogue_noise, rir, mode='same')\n",
        "        env_reverb = torchaudio.functional.fftconvolve(env_noise, rir, mode='same')\n",
        "\n",
        "        # 3. 믹싱\n",
        "        target_clean = target_reverb\n",
        "        target_noise = mix_audio(env_reverb, dialogue_reverb, random.uniform(-3, 3))\n",
        "        mixture = mix_audio(target_clean, target_noise, random.uniform(-5, 10))\n",
        "\n",
        "        # 4. 정규화\n",
        "        max_val = torch.max(torch.abs(mixture))\n",
        "        if max_val > 0: mixture = mixture / max_val\n",
        "\n",
        "        # 5. 반환 (Mixture, [Clean, Noise])\n",
        "        targets = torch.stack([target_clean.squeeze(0), target_noise.squeeze(0)], dim=0)\n",
        "        return mixture.squeeze(0), targets\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 5. 손실 함수 (V3 - STOI + SI-SNR)\n",
        "# -----------------------------------------------------------------------------\n",
        "stoi_loss_module = NegSTOILoss(sample_rate=SAMPLE_RATE).to(device)\n",
        "\n",
        "def si_snr_score(estimate, target, epsilon=EPSILON):\n",
        "    dot = torch.sum(estimate * target, dim=-1, keepdim=True)\n",
        "    target_norm_sq = torch.sum(target**2, dim=-1, keepdim=True)\n",
        "    target_scaled = (dot / (target_norm_sq + epsilon)) * target\n",
        "    noise = estimate - target_scaled\n",
        "    snr_sq = torch.sum(target_scaled**2, dim=-1) / (torch.sum(noise**2, dim=-1) + epsilon)\n",
        "    return 10 * torch.log10(snr_sq + epsilon).squeeze(-1)\n",
        "\n",
        "def pairwise_combined_loss_v3(estimates, targets, epsilon=EPSILON):\n",
        "    B, C_est, T = estimates.shape\n",
        "    _, C_tgt, _ = targets.shape\n",
        "\n",
        "    estimates_exp = estimates.unsqueeze(2).expand(B, C_est, C_tgt, T)\n",
        "    targets_exp = targets.unsqueeze(1).expand(B, C_est, C_tgt, T)\n",
        "\n",
        "    # SI-SNR Loss\n",
        "    loss_sisnr = -si_snr_score(estimates_exp, targets_exp, epsilon)\n",
        "\n",
        "    # STOI Loss\n",
        "    est_flat = estimates_exp.reshape(-1, T)\n",
        "    tgt_flat = targets_exp.reshape(-1, T)\n",
        "    loss_stoi = stoi_loss_module(est_flat, tgt_flat).reshape(B, C_est, C_tgt)\n",
        "\n",
        "    # L1 Loss (무음 구간용)\n",
        "    loss_l1 = torch.mean(torch.abs(estimates_exp - targets_exp), dim=-1)\n",
        "\n",
        "    # Masking\n",
        "    target_energy = torch.sum(targets**2, dim=-1)\n",
        "    is_silent = (target_energy < epsilon).unsqueeze(1).expand(B, C_est, C_tgt)\n",
        "\n",
        "    loss_speech = (LOSS_LAMBDA_V3 * loss_sisnr) + ((1 - LOSS_LAMBDA_V3) * loss_stoi)\n",
        "\n",
        "    return torch.where(is_silent, loss_l1, loss_speech)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 6. 모델 및 훈련 함수\n",
        "# -----------------------------------------------------------------------------\n",
        "def build_model_and_optimizer():\n",
        "    print(\"Pre-trained ConvTasNet 로드 중...\")\n",
        "    model = ConvTasNet.from_pretrained(\"JorisCos/ConvTasNet_Libri2Mix_sepnoisy_16k\")\n",
        "    model = model.to(device)\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-4)\n",
        "    loss_func = PITLossWrapper(pairwise_combined_loss_v3, pit_from='pw_mtx')\n",
        "    return model, optimizer, loss_func\n",
        "\n",
        "def train_epoch(model, loader, optimizer, loss_func):\n",
        "    model.train()\n",
        "    total_loss = 0.0\n",
        "    for mixture, targets in tqdm(loader, desc=\"[훈련]\"):\n",
        "        mixture, targets = mixture.to(device), targets.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        loss = loss_func(model(mixture), targets)\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5.0)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "def validate_epoch(model, loader, loss_func):\n",
        "    model.eval()\n",
        "    total_loss = 0.0\n",
        "    with torch.no_grad():\n",
        "        for mixture, targets in tqdm(loader, desc=\"[검증]\"):\n",
        "            mixture, targets = mixture.to(device), targets.to(device)\n",
        "            loss = loss_func(model(mixture), targets)\n",
        "            total_loss += loss.item()\n",
        "    return total_loss / len(loader)\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 7. 메인 실행 루프 (데이터 로더 생성 -> 훈련 -> 저장)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n=== 7. 메인 프로세스 시작 ===\")\n",
        "\n",
        "# A. 데이터셋 및 로더 준비\n",
        "train_dataset = SoribomDataset(TRAIN_DATA_PATH, sample_rate=SAMPLE_RATE,\n",
        "                               segment_duration_sec=SEGMENT_DURATION_SEC,\n",
        "                               mode='train', steps_per_epoch=TRAIN_STEPS_PER_EPOCH, batch_size=BATCH_SIZE)\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=2, pin_memory=True)\n",
        "\n",
        "val_dataset_full = SoribomDataset(VAL_DATA_PATH, sample_rate=SAMPLE_RATE,\n",
        "                                  segment_duration_sec=SEGMENT_DURATION_SEC,\n",
        "                                  mode='val', batch_size=BATCH_SIZE)\n",
        "\n",
        "# 검증 샘플 고정\n",
        "if MAX_VAL_SAMPLES and len(val_dataset_full) > MAX_VAL_SAMPLES:\n",
        "    print(f\"검증 샘플을 {MAX_VAL_SAMPLES}개로 제한합니다.\")\n",
        "    random.seed(42)\n",
        "    val_indices = random.sample(range(len(val_dataset_full)), MAX_VAL_SAMPLES)\n",
        "    val_dataset = Subset(val_dataset_full, val_indices)\n",
        "else:\n",
        "    val_dataset = val_dataset_full\n",
        "\n",
        "val_loader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
        "\n",
        "# B. 모델 준비\n",
        "model, optimizer, loss_func = build_model_and_optimizer()\n",
        "\n",
        "# C. 체크포인트 로드\n",
        "start_epoch = 1\n",
        "best_val_loss = float('inf')\n",
        "\n",
        "if os.path.exists(CHECKPOINT_PATH):\n",
        "    print(f\"체크포인트 로드: {CHECKPOINT_PATH}\")\n",
        "    ckpt = torch.load(CHECKPOINT_PATH, map_location=device)\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    optimizer.load_state_dict(ckpt['optimizer_state_dict'])\n",
        "    start_epoch = ckpt['epoch'] + 1\n",
        "    best_val_loss = ckpt.get('best_val_loss', float('inf'))\n",
        "    print(f\"-> Epoch {start_epoch}부터 시작 (Best Loss: {best_val_loss:.4f})\")\n",
        "else:\n",
        "    print(\"새로운 훈련 시작\")\n",
        "\n",
        "# D. 훈련 루프\n",
        "for epoch in range(start_epoch, NUM_EPOCHS + 1):\n",
        "    print(f\"\\n--- Epoch {epoch} / {NUM_EPOCHS} ---\")\n",
        "    train_loss = train_epoch(model, train_loader, optimizer, loss_func)\n",
        "    val_loss = validate_epoch(model, val_loader, loss_func)\n",
        "\n",
        "    print(f\"Epoch {epoch} 결과: Train={train_loss:.4f}, Val={val_loss:.4f}\")\n",
        "\n",
        "    # Best Model 저장\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
        "        print(f\"*** 최고 성능 갱신! 모델 저장: {BEST_MODEL_PATH}\")\n",
        "\n",
        "    # 체크포인트 저장\n",
        "    torch.save({\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimizer_state_dict': optimizer.state_dict(),\n",
        "        'best_val_loss': best_val_loss,\n",
        "    }, CHECKPOINT_PATH)\n",
        "    print(f\"체크포인트 저장됨.\")\n",
        "\n",
        "print(\"\\n모든 훈련이 완료되었습니다.\")\n",
        "\n",
        "# -----------------------------------------------------------------------------\n",
        "# 8. 최종 성능 평가 (SI-SNR, SI-SNRi, STOI)\n",
        "# -----------------------------------------------------------------------------\n",
        "print(\"\\n=== 8. 최종 성능 평가 시작 (Best Model 사용) ===\")\n",
        "\n",
        "# 최고 성능 모델 로드\n",
        "if os.path.exists(BEST_MODEL_PATH):\n",
        "    model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
        "    print(f\"평가를 위해 최고 성능 모델 로드 완료: {BEST_MODEL_PATH}\")\n",
        "else:\n",
        "    print(\"[Warning] 최고 모델 파일이 없습니다. 현재 모델 상태로 평가를 진행합니다.\")\n",
        "\n",
        "model.eval()\n",
        "all_sisnr, all_sisnri, all_stoi = [], [], []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for mixture, targets in tqdm(val_loader, desc=\"[최종 평가]\"):\n",
        "        mixture, targets = mixture.to(device), targets.to(device)\n",
        "        target_clean = targets[:, 0, :]\n",
        "\n",
        "        estimated_sources = model(mixture)\n",
        "        est_1, est_2 = estimated_sources[:, 0, :], estimated_sources[:, 1, :]\n",
        "\n",
        "        # Permutation Invariant 해결 (더 높은 SI-SNR을 가진 소스를 clean으로 간주)\n",
        "        sisnr_1 = si_snr_score(est_1, target_clean)\n",
        "        sisnr_2 = si_snr_score(est_2, target_clean)\n",
        "\n",
        "        mask = (sisnr_1 > sisnr_2).unsqueeze(1)\n",
        "        best_estimate = torch.where(mask.expand_as(est_1), est_1, est_2)\n",
        "\n",
        "        # Metrics\n",
        "        output_sisnr = torch.max(sisnr_1, sisnr_2)\n",
        "        initial_sisnr = si_snr_score(mixture, target_clean)\n",
        "        sisnri = output_sisnr - initial_sisnr\n",
        "        stoi_val = -stoi_loss_module(best_estimate, target_clean)\n",
        "\n",
        "        all_sisnr.extend(output_sisnr.cpu().numpy())\n",
        "        all_sisnri.extend(sisnri.cpu().numpy())\n",
        "        all_stoi.extend(stoi_val.cpu().numpy())\n",
        "\n",
        "print(\"\\n================ 최종 성능 평가 결과 ================\")\n",
        "print(f\" 1. SI-SNR (최종 음질)  : {np.mean(all_sisnr):.4f} (dB)\")\n",
        "print(f\" 2. SI-SNRi (음질 향상도): {np.mean(all_sisnri):.4f} (dB)\")\n",
        "print(f\" 3. STOI (명료도)       : {np.mean(all_stoi):.4f}\")\n",
        "print(\"===================================================\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
