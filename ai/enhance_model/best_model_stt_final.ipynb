{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ee6NM3v90LHS"
      },
      "outputs": [],
      "source": [
        "!pip install asteroid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sf3Nr6Hb1E9Y"
      },
      "outputs": [],
      "source": [
        "!pip install torchcodec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MOPCwXb90RCq"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qVz3gfkT0S2A"
      },
      "outputs": [],
      "source": [
        "print(\"구글 드라이브에서 코랩 로컬 디스크로 데이터 복사 중...\")\n",
        "!mkdir -p /content/realdata\n",
        "!cp -r /content/drive/MyDrive/dprnn_project/train /content/realdata/\n",
        "!cp -r /content/drive/MyDrive/dprnn_project/val /content/realdata/\n",
        "print(\"데이터 복사 완료.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ctw_rvQR0U7V"
      },
      "outputs": [],
      "source": [
        "# -----------------------------------------------------------------\n",
        "# 지하철 안내방송 STT 최적화 학습 코드 (Complete Version)\n",
        "# - 타깃: Clean (무잔향, 0%)\n",
        "# - 입력: 강한 잔향 (100%) + 소음\n",
        "# - 전처리: 100Hz High-pass Filter (저음 웅웅거림 제거)\n",
        "# - 손실함수: Multi-Resolution STFT + SI-SNR + STOI\n",
        "# -----------------------------------------------------------------\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import scipy.signal\n",
        "import torch\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from tqdm import tqdm\n",
        "from asteroid.models import BaseModel\n",
        "from torch_stoi import NegSTOILoss\n",
        "import pyroomacoustics as pra\n",
        "import traceback\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# [PART 1] 지하철 특화 RIR 생성 함수들\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "def generate_subway_car_rir(sr=16000):\n",
        "    \"\"\" 지하철 객차 RIR \"\"\"\n",
        "    try:\n",
        "        length = float(np.random.uniform(15, 20))\n",
        "        width = float(np.random.uniform(2.8, 3.2))\n",
        "        height = float(np.random.uniform(2.3, 2.5))\n",
        "        room_dim = [length, width, height]\n",
        "\n",
        "        e_absorption = float(np.random.uniform(0.15, 0.25))\n",
        "\n",
        "        room = pra.ShoeBox(\n",
        "            room_dim, fs=int(sr), materials=pra.Material(e_absorption), max_order=3\n",
        "        )\n",
        "\n",
        "        source_pos = np.array([\n",
        "            np.random.uniform(2, length - 2),\n",
        "            np.random.uniform(0.5, width - 0.5),\n",
        "            np.random.uniform(2.0, height - 0.1)\n",
        "        ], dtype=float).reshape(3, 1)\n",
        "\n",
        "        mic_pos = np.array([\n",
        "            np.random.uniform(2, length - 2),\n",
        "            np.random.uniform(0.5, width - 0.5),\n",
        "            np.random.uniform(1.2, 1.7)\n",
        "        ], dtype=float).reshape(3, 1)\n",
        "\n",
        "        room.add_source(source_pos[:, 0].tolist())\n",
        "        mic_array = pra.MicrophoneArray(mic_pos, room.fs)\n",
        "        room.add_microphone_array(mic_array)\n",
        "\n",
        "        room.compute_rir()\n",
        "        rir = room.rir[0][0]\n",
        "        return np.array(rir, dtype=float)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"generate_subway_car_rir error: {e}\")\n",
        "        rir = np.zeros(256, dtype=float)\n",
        "        rir[0] = 1.0\n",
        "        return rir\n",
        "\n",
        "def generate_subway_platform_rir(sr=16000):\n",
        "    \"\"\" 지하철 플랫폼 RIR \"\"\"\n",
        "    try:\n",
        "        length = float(np.random.uniform(100, 150))\n",
        "        width = float(np.random.uniform(10, 15))\n",
        "        height = float(np.random.uniform(3.5, 4.5))\n",
        "        room_dim = [length, width, height]\n",
        "\n",
        "        e_absorption = float(np.random.uniform(0.05, 0.12))\n",
        "\n",
        "        room = pra.ShoeBox(\n",
        "            room_dim, fs=int(sr), materials=pra.Material(e_absorption), max_order=5\n",
        "        )\n",
        "\n",
        "        source_pos = np.array([\n",
        "            np.random.uniform(10, length - 10),\n",
        "            np.random.uniform(2, width - 2),\n",
        "            np.random.uniform(3.0, height - 0.1)\n",
        "        ], dtype=float).reshape(3, 1)\n",
        "\n",
        "        mic_pos = np.array([\n",
        "            np.random.uniform(10, length - 10),\n",
        "            np.random.uniform(2, width - 2),\n",
        "            np.random.uniform(1.4, 1.8)\n",
        "        ], dtype=float).reshape(3, 1)\n",
        "\n",
        "        room.add_source(source_pos[:, 0].tolist())\n",
        "        mic_array = pra.MicrophoneArray(mic_pos, room.fs)\n",
        "        room.add_microphone_array(mic_array)\n",
        "\n",
        "        room.compute_rir()\n",
        "        rir = room.rir[0][0]\n",
        "        return np.array(rir, dtype=float)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"generate_subway_platform_rir error: {e}\")\n",
        "        rir = np.zeros(256, dtype=float)\n",
        "        rir[0] = 1.0\n",
        "        return rir\n",
        "\n",
        "def generate_subway_corridor_rir(sr=16000):\n",
        "    \"\"\" 지하철 통로 RIR \"\"\"\n",
        "    try:\n",
        "        length = float(np.random.uniform(30, 50))\n",
        "        width = float(np.random.uniform(3, 5))\n",
        "        height = float(np.random.uniform(2.5, 3.0))\n",
        "        room_dim = [length, width, height]\n",
        "\n",
        "        e_absorption = float(np.random.uniform(0.10, 0.18))\n",
        "\n",
        "        room = pra.ShoeBox(\n",
        "            room_dim, fs=int(sr), materials=pra.Material(e_absorption), max_order=4\n",
        "        )\n",
        "\n",
        "        source_pos = np.array([\n",
        "            np.random.uniform(5, length - 5),\n",
        "            np.random.uniform(0.5, width - 0.5),\n",
        "            np.random.uniform(2.0, height - 0.1)\n",
        "        ], dtype=float).reshape(3, 1)\n",
        "\n",
        "        mic_pos = np.array([\n",
        "            np.random.uniform(5, length - 5),\n",
        "            np.random.uniform(0.5, width - 0.5),\n",
        "            np.random.uniform(1.4, 1.7)\n",
        "        ], dtype=float).reshape(3, 1)\n",
        "\n",
        "        room.add_source(source_pos[:, 0].tolist())\n",
        "        mic_array = pra.MicrophoneArray(mic_pos, room.fs)\n",
        "        room.add_microphone_array(mic_array)\n",
        "\n",
        "        room.compute_rir()\n",
        "        rir = room.rir[0][0]\n",
        "        return np.array(rir, dtype=float)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"generate_subway_corridor_rir error: {e}\")\n",
        "        rir = np.zeros(256, dtype=float)\n",
        "        rir[0] = 1.0\n",
        "        return rir\n",
        "\n",
        "def generate_random_subway_rir(sr=16000):\n",
        "    \"\"\" 다양한 지하철 환경 RIR 랜덤 선택 \"\"\"\n",
        "    rir_funcs = [generate_subway_car_rir, generate_subway_platform_rir, generate_subway_corridor_rir]\n",
        "    selected_func = random.choice(rir_funcs)\n",
        "    return selected_func(sr)\n",
        "\n",
        "def apply_rir_split(waveform, rir, sr=16000, early_ms=50):\n",
        "    \"\"\"\n",
        "    [핵심 수정 함수]\n",
        "    RIR을 적용하되, 시간 지연(Delay) 문제를 해결하기 위해\n",
        "    Input용(Full Reverb)과 Target용(Early Reverb)을 나누어 반환\n",
        "\n",
        "    Args:\n",
        "        waveform: 원본 음성 (Clean)\n",
        "        rir: 적용할 RIR\n",
        "        sr: 샘플링 레이트\n",
        "        early_ms: 타깃에 남길 초기 반사음 길이 (ms). 50ms로 설정.\n",
        "                  (이 값이 0이면 완전 Clean이지만, 약간의 공간감을 남겨야 학습이 더 잘됨)\n",
        "\n",
        "    Returns:\n",
        "        out_full: 잔향이 100% 적용된 오디오 (Input용)\n",
        "        out_early: 초기 반사음만 적용되어 타이밍이 정렬된 오디오 (Target용)\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # 1. 텐서/넘파이 변환\n",
        "        if isinstance(waveform, torch.Tensor):\n",
        "            wav_np = waveform.detach().cpu().squeeze().numpy().astype(np.float32)\n",
        "        else:\n",
        "            wav_np = np.array(waveform).squeeze().astype(np.float32)\n",
        "\n",
        "        # 2. RIR 에너지 정규화\n",
        "        rir_energy = np.sqrt(np.sum(rir**2) + 1e-8)\n",
        "        rir = rir / rir_energy\n",
        "\n",
        "        # 3. Early RIR (Target용) 생성 로직\n",
        "        # RIR에서 가장 에너지가 큰 지점(Direct Sound)을 찾습니다.\n",
        "        peak_idx = np.argmax(np.abs(rir))\n",
        "\n",
        "        # 피크부터 early_ms(50ms) 만큼의 샘플 수 계산\n",
        "        early_samples = int(sr * (early_ms / 1000.0))\n",
        "\n",
        "        # Early RIR 생성 (꼬리 자르기)\n",
        "        rir_early = np.zeros_like(rir)\n",
        "        end_idx = min(len(rir), peak_idx + early_samples)\n",
        "        rir_early[:end_idx] = rir[:end_idx] # 피크 포함 앞부분만 복사\n",
        "\n",
        "        # 4. Convolution (Full vs Early)\n",
        "        # mode='full'로 해야 시작점(t=0)이 왜곡되지 않습니다.\n",
        "        mix_full = scipy.signal.fftconvolve(wav_np, rir, mode='full')\n",
        "        mix_early = scipy.signal.fftconvolve(wav_np, rir_early, mode='full')\n",
        "\n",
        "        # 5. 원본 길이로 자르기\n",
        "        src_len = len(wav_np)\n",
        "        mix_full = mix_full[:src_len]\n",
        "        mix_early = mix_early[:src_len]\n",
        "\n",
        "        # 6. Clipping (소리 깨짐 방지)\n",
        "        mix_full = np.clip(mix_full, -1.0, 1.0)\n",
        "        mix_early = np.clip(mix_early, -1.0, 1.0)\n",
        "\n",
        "        # 7. 텐서 변환\n",
        "        out_full = torch.from_numpy(mix_full).float().unsqueeze(0)\n",
        "        out_early = torch.from_numpy(mix_early).float().unsqueeze(0)\n",
        "\n",
        "        return out_full, out_early\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"apply_rir_split error: {e}\")\n",
        "        # 에러 발생 시 원본을 그대로 반환 (차원 맞춰서)\n",
        "        dummy = waveform if isinstance(waveform, torch.Tensor) else torch.tensor(waveform).float()\n",
        "        if dummy.dim() == 1: dummy = dummy.unsqueeze(0)\n",
        "        return dummy, dummy\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# [PART 2] Babble Noise 생성\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "def create_babble_noise(speech_files, target_len, sr=16000, num_speakers_range=(3, 7)):\n",
        "    if not speech_files:\n",
        "        return torch.zeros(1, target_len)\n",
        "\n",
        "    num_speakers = random.randint(*num_speakers_range)\n",
        "    if len(speech_files) < num_speakers:\n",
        "        selected_files = random.choices(speech_files, k=num_speakers)\n",
        "    else:\n",
        "        selected_files = random.sample(speech_files, num_speakers)\n",
        "\n",
        "    mixed_babble = torch.zeros(1, target_len)\n",
        "    speaker_count = 0\n",
        "\n",
        "    for file_path in selected_files:\n",
        "        try:\n",
        "            wav, file_sr = torchaudio.load(file_path)\n",
        "            if file_sr != sr:\n",
        "                resampler = T.Resample(file_sr, sr)\n",
        "                wav = resampler(wav)\n",
        "            if wav.dim() == 2 and wav.size(0) > 1:\n",
        "                wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "            if wav.size(1) > target_len:\n",
        "                start = random.randint(0, wav.size(1) - target_len)\n",
        "                chunk = wav[:, start : start + target_len]\n",
        "            else:\n",
        "                chunk = F.pad(wav, (0, max(0, target_len - wav.size(1))))\n",
        "\n",
        "            speed_change = random.uniform(0.9, 1.1)\n",
        "            if abs(speed_change - 1.0) > 1e-6:\n",
        "                new_len = max(1, int(target_len / speed_change))\n",
        "                chunk = F.interpolate(chunk.unsqueeze(0), size=new_len, mode='linear', align_corners=False).squeeze(0)\n",
        "                if chunk.size(1) > target_len:\n",
        "                    chunk = chunk[:, :target_len]\n",
        "                else:\n",
        "                    chunk = F.pad(chunk, (0, target_len - chunk.size(1)))\n",
        "\n",
        "            chunk_rms = torch.sqrt(torch.mean(chunk**2) + 1e-8)\n",
        "            chunk = chunk / (chunk_rms + 1e-8)\n",
        "\n",
        "            gain = random.uniform(0.5, 1.0)\n",
        "            mixed_babble += (chunk * gain)\n",
        "            speaker_count += 1\n",
        "\n",
        "        except Exception as e:\n",
        "            continue\n",
        "\n",
        "    if speaker_count > 0:\n",
        "        mixed_babble = mixed_babble / speaker_count\n",
        "\n",
        "    max_val = torch.max(torch.abs(mixed_babble))\n",
        "    if max_val > 1e-6:\n",
        "        mixed_babble = mixed_babble / (max_val + 1e-8) * 0.9\n",
        "\n",
        "    return mixed_babble\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# [PART 3] 손실 함수 (Multi-Resolution STFT + SI-SNR + STOI)\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "SAMPLE_RATE = 16000\n",
        "\n",
        "# 1. Multi-Resolution STFT Loss Class\n",
        "class MultiResolutionSTFTLoss(nn.Module):\n",
        "    def __init__(self, fft_sizes=[1024, 2048, 512], hop_sizes=[120, 240, 60], win_lengths=[600, 1200, 240]):\n",
        "        super().__init__()\n",
        "        self.fft_sizes = fft_sizes\n",
        "        self.hop_sizes = hop_sizes\n",
        "        self.win_lengths = win_lengths\n",
        "\n",
        "    def stft(self, x, fft_size, hop_size, win_length):\n",
        "        window = torch.hann_window(win_length).to(x.device)\n",
        "        return torch.stft(x, n_fft=fft_size, hop_length=hop_size, win_length=win_length,\n",
        "                          window=window, return_complex=True)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        # x: estimate, y: target\n",
        "        loss = 0.0\n",
        "        for fs, hs, wl in zip(self.fft_sizes, self.hop_sizes, self.win_lengths):\n",
        "            x_stft = self.stft(x, fs, hs, wl)\n",
        "            y_stft = self.stft(y, fs, hs, wl)\n",
        "\n",
        "            x_mag = torch.abs(x_stft) + 1e-7\n",
        "            y_mag = torch.abs(y_stft) + 1e-7\n",
        "\n",
        "            # Spectral Convergence Loss\n",
        "            sc_loss = torch.norm(y_mag - x_mag, p=\"fro\") / (torch.norm(y_mag, p=\"fro\") + 1e-7)\n",
        "            # Log Magnitude Loss\n",
        "            mag_loss = F.l1_loss(torch.log(y_mag), torch.log(x_mag))\n",
        "\n",
        "            loss += sc_loss + mag_loss\n",
        "        return loss / len(self.fft_sizes)\n",
        "\n",
        "# 손실함수 인스턴스 (Main Loop에서 device로 보냄)\n",
        "mr_stft_loss_func = MultiResolutionSTFTLoss()\n",
        "stoi_loss_func = NegSTOILoss(sample_rate=SAMPLE_RATE)\n",
        "\n",
        "def si_snr(estimate, target, epsilon=1e-8):\n",
        "    if estimate.dim() == 1:\n",
        "        estimate = estimate.unsqueeze(0)\n",
        "    if target.dim() == 1:\n",
        "        target = target.unsqueeze(0)\n",
        "\n",
        "    estimate_z = estimate - torch.mean(estimate, dim=-1, keepdim=True)\n",
        "    target_z = target - torch.mean(target, dim=-1, keepdim=True)\n",
        "\n",
        "    dot = torch.sum(estimate_z * target_z, dim=-1, keepdim=True)\n",
        "    target_norm_sq = torch.sum(target_z**2, dim=-1, keepdim=True)\n",
        "\n",
        "    target_scaled = (dot / (target_norm_sq + epsilon)) * target_z\n",
        "    noise = estimate_z - target_scaled\n",
        "\n",
        "    snr_sq = torch.sum(target_scaled**2, dim=-1) / (torch.sum(noise**2, dim=-1) + epsilon)\n",
        "    si_snr_score = 10 * torch.log10(snr_sq + epsilon)\n",
        "\n",
        "    return si_snr_score.squeeze()\n",
        "\n",
        "def combined_loss_stt_optimized(estimate, target, epsilon=1e-8):\n",
        "    \"\"\"\n",
        "    STT 최적화 손실함수\n",
        "    - STFT Loss: 스펙트럼(발음) 복원 (가장 중요)\n",
        "    - SI-SNR: 파형 정렬\n",
        "    - STOI: 명료도 향상\n",
        "    \"\"\"\n",
        "    # 1. Spectral Loss (Magnitude)\n",
        "    loss_freq = mr_stft_loss_func(estimate, target)\n",
        "\n",
        "    # 2. SI-SNR Loss\n",
        "    snr = si_snr(estimate, target, epsilon)\n",
        "    loss_sisnr = -torch.mean(snr)\n",
        "\n",
        "    # 3. STOI Loss\n",
        "    loss_stoi = torch.mean(stoi_loss_func(estimate, target))\n",
        "\n",
        "    # 4. Target이 무음인지 확인 (무음 구간은 L1 Loss로 억제)\n",
        "    target_energy = torch.sum(target**2, dim=-1)\n",
        "    is_silent_mask = (target_energy < 1e-6)\n",
        "\n",
        "    # 가중치: STFT(0.5)로 질감을 잡고, STOI(0.3)로 명료도, SI-SNR(0.2)로 파형 보정\n",
        "    loss_speech = (0.5 * loss_freq) + (0.2 * loss_sisnr) + (0.3 * loss_stoi)\n",
        "\n",
        "    loss_l1 = F.l1_loss(estimate, target)\n",
        "\n",
        "    final_loss = torch.where(\n",
        "        is_silent_mask,\n",
        "        loss_l1,\n",
        "        loss_speech\n",
        "    )\n",
        "\n",
        "    return torch.mean(final_loss)\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# [PART 4] Dataset (High-pass Filter + Clean Target)\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "class SubwaySTTDataset(Dataset):\n",
        "    def __init__(\n",
        "        self,\n",
        "        announcement_dir,\n",
        "        noise_dir,\n",
        "        speech_dir,\n",
        "        sample_rate=16000,\n",
        "        chunk_seconds=10.0,\n",
        "        snr_range=(-5.0, 5.0),\n",
        "        epoch_len=2000,\n",
        "        noise_only_ratio=0.1,\n",
        "        target_rir_strength=0.0,\n",
        "        input_rir_strength=1.0,\n",
        "        is_validation=False\n",
        "    ):\n",
        "        self.announcement_files = self._get_files(announcement_dir)\n",
        "        self.noise_files = self._get_files(noise_dir)\n",
        "        self.speech_files = self._get_files(speech_dir) if speech_dir else []\n",
        "        self.has_speech = len(self.speech_files) > 0\n",
        "\n",
        "        self.sample_rate = int(sample_rate)\n",
        "        self.chunk_seconds = float(chunk_seconds)\n",
        "        self.chunk_samples = int(self.chunk_seconds * self.sample_rate)\n",
        "        self.snr_range = snr_range\n",
        "        self.epoch_len = int(epoch_len)\n",
        "        self.noise_only_ratio = float(noise_only_ratio)\n",
        "        self.target_rir_strength = float(target_rir_strength)\n",
        "        self.input_rir_strength = float(input_rir_strength)\n",
        "        self.is_validation = is_validation\n",
        "\n",
        "        # High-pass Filter (100Hz 이하 제거 - 지하철 웅웅거림 삭제)\n",
        "        # 로딩 속도를 위해 _load_wav_chunk에서 직접 biquad 적용\n",
        "\n",
        "        assert len(self.noise_files) > 0, f\"Noise 파일 없음: {noise_dir}\"\n",
        "        assert len(self.announcement_files) > 0, f\"Announcement 파일 없음: {announcement_dir}\"\n",
        "\n",
        "    def _get_files(self, path):\n",
        "        if path and os.path.exists(path):\n",
        "            return [os.path.join(path, f) for f in os.listdir(path) if f.lower().endswith((\".mp3\", \".wav\"))]\n",
        "        return []\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.epoch_len\n",
        "\n",
        "    def _load_wav_chunk(self, path, length):\n",
        "        try:\n",
        "            wav, file_sr = torchaudio.load(path)\n",
        "\n",
        "            if file_sr != self.sample_rate:\n",
        "                wav = T.Resample(file_sr, self.sample_rate)(wav)\n",
        "\n",
        "            if wav.dim() == 2 and wav.size(0) > 1:\n",
        "                wav = wav.mean(dim=0, keepdim=True)\n",
        "\n",
        "            # 로드 시점에 High-pass Filter 적용 (100Hz)\n",
        "            # STT 인식률을 위해 불필요한 저음 노이즈 제거\n",
        "            wav = torchaudio.functional.highpass_biquad(wav, self.sample_rate, cutoff_freq=100)\n",
        "\n",
        "            if wav.size(1) > length:\n",
        "                start = random.randint(0, wav.size(1) - length)\n",
        "                wav = wav[:, start:start + length]\n",
        "            elif wav.size(1) < length:\n",
        "                wav = F.pad(wav, (0, length - wav.size(1)))\n",
        "\n",
        "            return wav\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"_load_wav_chunk failed for {path}: {e}\")\n",
        "            return torch.zeros(1, length)\n",
        "\n",
        "    def _scale_mix(self, clean, noise, snr_db):\n",
        "        rms_c = torch.sqrt(torch.mean(clean ** 2) + 1e-8)\n",
        "        rms_n = torch.sqrt(torch.mean(noise ** 2) + 1e-8)\n",
        "        scale = (rms_c / (10 ** (snr_db / 20))) / (rms_n + 1e-8)\n",
        "        return noise * scale\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if self.is_validation:\n",
        "            seed = idx\n",
        "            random.seed(seed)\n",
        "            np.random.seed(seed)\n",
        "            torch.manual_seed(seed)\n",
        "\n",
        "        is_announcement = random.random() > self.noise_only_ratio\n",
        "\n",
        "        # 1. 안내방송 로드 (HPF 적용됨)\n",
        "        if is_announcement:\n",
        "            ann_path = random.choice(self.announcement_files)\n",
        "            clean_ann = self._load_wav_chunk(ann_path, self.chunk_samples)\n",
        "\n",
        "            # RIR 생성\n",
        "            shared_rir = generate_random_subway_rir(self.sample_rate)\n",
        "\n",
        "            # apply_rir_split 사용\n",
        "            # Input: Full Reverb (잔향 100%)\n",
        "            # Target: Early Reverb (초기반사음만 남김 -> 타이밍 정렬됨 + 깨끗함)\n",
        "            input_ann, target_chunk = apply_rir_split(\n",
        "                clean_ann, shared_rir, sr=self.sample_rate, early_ms=50\n",
        "            )\n",
        "\n",
        "            target_rms = torch.sqrt(torch.mean(target_chunk ** 2)).squeeze()\n",
        "        else:\n",
        "            target_chunk = torch.zeros(1, self.chunk_samples)\n",
        "            input_ann = torch.zeros_like(target_chunk)\n",
        "            target_rms = torch.tensor(0.0)\n",
        "\n",
        "        # 2. 소음 준비 (HPF 적용됨)\n",
        "        r_case = random.random()\n",
        "        env_path = random.choice(self.noise_files)\n",
        "        noise_env = self._load_wav_chunk(env_path, self.chunk_samples)\n",
        "\n",
        "        noise_speech = torch.zeros_like(noise_env)\n",
        "        if self.has_speech:\n",
        "            noise_speech = create_babble_noise(self.speech_files, self.chunk_samples, self.sample_rate)\n",
        "\n",
        "            # Babble Noise도 apply_rir_split으로 처리하되, Full(noise_input)만 사용\n",
        "            noise_rir = generate_random_subway_rir(self.sample_rate)\n",
        "            noise_speech, _ = apply_rir_split(noise_speech, noise_rir, sr=self.sample_rate)\n",
        "\n",
        "        if r_case < 0.4:\n",
        "            final_noise = noise_env\n",
        "        elif r_case < 0.9:\n",
        "            final_noise = noise_env + (noise_speech * random.uniform(0.5, 1.2))\n",
        "        else:\n",
        "            final_noise = noise_speech\n",
        "\n",
        "        # 3. 믹싱\n",
        "        if is_announcement:\n",
        "            snr_db = random.uniform(*self.snr_range)\n",
        "            noise_scaled = self._scale_mix(input_ann, final_noise, snr_db)\n",
        "            mixture = input_ann + noise_scaled\n",
        "        else:\n",
        "            rms_noise = torch.sqrt(torch.mean(final_noise ** 2) + 1e-8)\n",
        "            if rms_noise > 0.0:\n",
        "                mixture = final_noise * (random.uniform(0.5, 1.0) / rms_noise)\n",
        "            else:\n",
        "                mixture = final_noise\n",
        "\n",
        "        # 4. Normalization (Peak)\n",
        "        max_val = torch.max(torch.abs(mixture))\n",
        "        if max_val > 0.99:\n",
        "            scale = 0.99 / max_val\n",
        "            mixture = mixture * scale\n",
        "            target_chunk = target_chunk * scale\n",
        "            target_rms = target_rms * scale\n",
        "\n",
        "        # 5. RMS Norm\n",
        "        eps = 1e-8\n",
        "        mixture_rms = torch.sqrt(torch.mean(mixture**2) + eps)\n",
        "        target_rms_val = torch.sqrt(torch.mean(target_chunk**2) + eps)\n",
        "\n",
        "        mixture = mixture / mixture_rms\n",
        "        target_chunk = target_chunk / target_rms_val\n",
        "        target_rms = target_rms / (target_rms_val + eps)\n",
        "\n",
        "        return mixture.squeeze(0), target_chunk.squeeze(0), target_rms\n",
        "\n",
        "# -----------------------------------------------------------------\n",
        "# [PART 5] 학습 및 검증 Main Loop\n",
        "# -----------------------------------------------------------------\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "BASE_DRIVE_PATH = \"/content/realdata\"\n",
        "MODEL_SAVE_PATH = \"/content/drive/MyDrive/dprnn_project/realcheckpoints_stt_opt\"\n",
        "os.makedirs(MODEL_SAVE_PATH, exist_ok=True)\n",
        "\n",
        "BATCH_SIZE = 4\n",
        "LEARNING_RATE = 5e-5\n",
        "EPOCHS = 100\n",
        "TRAIN_EPOCH_LEN = 2000\n",
        "VAL_EPOCH_LEN = 200\n",
        "LOAD_CHECKPOINT = True\n",
        "\n",
        "def main_train():\n",
        "    print(f\"사용 장치: {device}\")\n",
        "    print(\"타깃: Clean (무잔향 0%)\")\n",
        "    print(\"입력: 강한 잔향 (100%) + 소음 + HPF Filter(100Hz)\")\n",
        "    print(\"손실함수: STFT(0.5) + SI-SNR(0.2) + STOI(0.3)\")\n",
        "\n",
        "    # Dataset 설정: target_rir_strength=0.0 (Clean)\n",
        "    train_dataset = SubwaySTTDataset(\n",
        "        f\"{BASE_DRIVE_PATH}/train/announcement_clean\",\n",
        "        f\"{BASE_DRIVE_PATH}/train/noise_environment\",\n",
        "        f\"{BASE_DRIVE_PATH}/train/noise_speech\",\n",
        "        epoch_len=TRAIN_EPOCH_LEN,\n",
        "        noise_only_ratio=0.1,\n",
        "        target_rir_strength=0.0,  # 중요: Clean Target\n",
        "        input_rir_strength=1.0\n",
        "    )\n",
        "    val_dataset = SubwaySTTDataset(\n",
        "        f\"{BASE_DRIVE_PATH}/val/announcement_clean\",\n",
        "        f\"{BASE_DRIVE_PATH}/val/noise_environment\",\n",
        "        f\"{BASE_DRIVE_PATH}/val/noise_speech\",\n",
        "        epoch_len=VAL_EPOCH_LEN,\n",
        "        noise_only_ratio=0.1,\n",
        "        target_rir_strength=0.0,  # 중요: Clean Target\n",
        "        input_rir_strength=1.0,\n",
        "        is_validation=True\n",
        "    )\n",
        "\n",
        "    import platform\n",
        "    num_workers = 0 if \"COLAB\" in os.environ or platform.system() == \"Windows\" else 2\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
        "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset, batch_size=BATCH_SIZE, shuffle=False,\n",
        "        num_workers=num_workers, pin_memory=torch.cuda.is_available()\n",
        "    )\n",
        "\n",
        "    # Model\n",
        "    print(\"-> Pre-trained Model 로드 중...\")\n",
        "    model = BaseModel.from_pretrained(\"JorisCos/DCCRNet_Libri1Mix_enhsingle_16k\").to(device)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5)\n",
        "\n",
        "    # Loss functions to device\n",
        "    global mr_stft_loss_func, stoi_loss_func\n",
        "    mr_stft_loss_func = mr_stft_loss_func.to(device)\n",
        "    stoi_loss_func = stoi_loss_func.to(device)\n",
        "\n",
        "    best_val_loss = float('inf')\n",
        "    start_epoch = 1\n",
        "\n",
        "    if LOAD_CHECKPOINT:\n",
        "        checkpoint_files = [f for f in os.listdir(MODEL_SAVE_PATH) if f.startswith(\"checkpoint_epoch_\")]\n",
        "        if checkpoint_files:\n",
        "            try:\n",
        "                latest_epoch = max([int(f.split('_')[-1].split('.')[0]) for f in checkpoint_files])\n",
        "                latest_checkpoint_path = os.path.join(MODEL_SAVE_PATH, f\"checkpoint_epoch_{latest_epoch}.pth\")\n",
        "\n",
        "                checkpoint = torch.load(latest_checkpoint_path, map_location=device)\n",
        "                model.load_state_dict(checkpoint['model_state_dict'])\n",
        "                optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "                scheduler.load_state_dict(checkpoint['scheduler_state_dict'])\n",
        "                best_val_loss = checkpoint['best_val_loss']\n",
        "                start_epoch = checkpoint['epoch'] + 1\n",
        "\n",
        "                print(f\"체크포인트 로드: Epoch {checkpoint['epoch']} → {start_epoch}부터 재개\")\n",
        "            except Exception as e:\n",
        "                print(f\"체크포인트 로드 실패: {e}\")\n",
        "\n",
        "    # Training Loop\n",
        "    for epoch in range(start_epoch, EPOCHS + 1):\n",
        "        # 1. Training\n",
        "        model.train()\n",
        "        train_loss_acc = 0.0\n",
        "\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch} [Train]\")\n",
        "        for mixture, target, target_rms in pbar:\n",
        "            mixture = mixture.to(device)\n",
        "            target = target.to(device)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            model_in = mixture.unsqueeze(1) if mixture.dim() == 2 else mixture\n",
        "            estimated = model(model_in)\n",
        "\n",
        "            if isinstance(estimated, dict):\n",
        "                estimated = estimated.get(\"waveform\", list(estimated.values())[0])\n",
        "            elif isinstance(estimated, (list, tuple)):\n",
        "                estimated = estimated[0]\n",
        "            if estimated.dim() == 3 and estimated.size(1) == 1:\n",
        "                estimated = estimated.squeeze(1)\n",
        "\n",
        "            # 변경된 손실함수 사용\n",
        "            loss = combined_loss_stt_optimized(estimated, target)\n",
        "\n",
        "            loss.backward()\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 5.0)\n",
        "            optimizer.step()\n",
        "\n",
        "            train_loss_acc += loss.item()\n",
        "            pbar.set_postfix(loss=f\"{loss.item():.4f}\")\n",
        "\n",
        "        # 2. Validation\n",
        "        model.eval()\n",
        "        val_loss_acc = 0.0\n",
        "        sisnr_sum, sisnri_sum, stoi_sum = 0.0, 0.0, 0.0\n",
        "        speech_sample_count = 0\n",
        "        val_batches = 0\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for mixture, target, target_rms in tqdm(val_loader, desc=f\"Epoch {epoch} [Val]\"):\n",
        "                mixture = mixture.to(device)\n",
        "                target = target.to(device)\n",
        "                target_rms = target_rms.to(device)\n",
        "\n",
        "                model_in = mixture.unsqueeze(1) if mixture.dim() == 2 else mixture\n",
        "                estimated = model(model_in)\n",
        "\n",
        "                if isinstance(estimated, dict):\n",
        "                    estimated = estimated.get(\"waveform\", list(estimated.values())[0])\n",
        "                elif isinstance(estimated, (list, tuple)):\n",
        "                    estimated = estimated[0]\n",
        "                if estimated.dim() == 3 and estimated.size(1) == 1:\n",
        "                    estimated = estimated.squeeze(1)\n",
        "\n",
        "                # Validation Loss\n",
        "                loss = combined_loss_stt_optimized(estimated, target)\n",
        "                val_loss_acc += loss.item()\n",
        "\n",
        "                # Metrics (Speech only)\n",
        "                is_speech_mask = (target_rms > 1e-6)\n",
        "                if is_speech_mask.any():\n",
        "                    speech_est = estimated[is_speech_mask]\n",
        "                    speech_target = target[is_speech_mask]\n",
        "                    speech_mix = mixture[is_speech_mask]\n",
        "\n",
        "                    # SI-SNR\n",
        "                    sisnr_est = si_snr(speech_est, speech_target)\n",
        "                    sisnr_init = si_snr(speech_mix, speech_target)\n",
        "\n",
        "                    sisnr_sum += float(torch.sum(sisnr_est).item())\n",
        "                    sisnri_sum += float(torch.sum(sisnr_est - sisnr_init).item())\n",
        "\n",
        "                    # STOI\n",
        "                    try:\n",
        "                        stoi_vals = -stoi_loss_func(speech_est, speech_target)\n",
        "                        if isinstance(stoi_vals, torch.Tensor):\n",
        "                            stoi_sum += float(torch.sum(stoi_vals).item())\n",
        "                        else:\n",
        "                            stoi_sum += float(stoi_vals)\n",
        "                    except:\n",
        "                        pass\n",
        "\n",
        "                    speech_sample_count += int(speech_target.size(0))\n",
        "                val_batches += 1\n",
        "\n",
        "        # Average computations\n",
        "        avg_loss = val_loss_acc / (val_batches if val_batches > 0 else 1)\n",
        "        if speech_sample_count > 0:\n",
        "            avg_sisnr = sisnr_sum / speech_sample_count\n",
        "            avg_sisnri = sisnri_sum / speech_sample_count\n",
        "            avg_stoi = stoi_sum / speech_sample_count\n",
        "        else:\n",
        "            avg_sisnr = avg_sisnri = avg_stoi = 0.0\n",
        "\n",
        "        print(f\"\\n[Epoch {epoch}] Results:\")\n",
        "        print(f\"    Total Loss: {avg_loss:.4f}\")\n",
        "        print(f\"    SI-SNR: {avg_sisnr:.2f} dB\")\n",
        "        print(f\"    SI-SNRi: {avg_sisnri:.2f} dB\")\n",
        "        print(f\"    STOI: {avg_stoi:.4f}\")\n",
        "\n",
        "        scheduler.step(avg_loss)\n",
        "        current_lr = optimizer.param_groups[0]['lr']\n",
        "        print(f\"    Current LR: {current_lr:.2e}\")\n",
        "\n",
        "        # Save Best Model (Loss 기준)\n",
        "        if avg_loss < best_val_loss:\n",
        "            best_val_loss = avg_loss\n",
        "            torch.save(model.state_dict(), os.path.join(MODEL_SAVE_PATH, \"best_model_stt_final.pth\"))\n",
        "            print(f\"    --> ★ Best Model Saved! (Loss: {avg_loss:.4f})\")\n",
        "\n",
        "        # Save Checkpoint\n",
        "        checkpoint_path = os.path.join(MODEL_SAVE_PATH, f\"checkpoint_epoch_{epoch}.pth\")\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'scheduler_state_dict': scheduler.state_dict(),\n",
        "            'best_val_loss': best_val_loss,\n",
        "        }, checkpoint_path)\n",
        "        print(f\"    Checkpoint saved: {checkpoint_path}\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main_train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
